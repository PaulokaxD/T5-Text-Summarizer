{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets==3.1.0\n",
    "!pip install accelerate==1.0.1\n",
    "!pip install transformers==4.46.3\n",
    "!pip install sentence-transformers==3.2.1\n",
    "!pip install rouge-score==0.1.2\n",
    "!pip install evaluate==0.4.3\n",
    "!pip install numpy==1.24.0\n",
    "!pip install scipy==1.9.3\n",
    "!pip install torch==2.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/home/4gomez/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (AutoTokenizer,\n",
    "                          DataCollatorForSeq2Seq,\n",
    "                          AutoModelForSeq2SeqLM,\n",
    "                          Seq2SeqTrainingArguments,\n",
    "                          Seq2SeqTrainer,\n",
    "                          EarlyStoppingCallback\n",
    "                          )\n",
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "T5 = \"google-t5/t5-small\"\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(T5)\n",
    "TEXT_N_TOKENS = 4000#5000#10000\n",
    "SUMMARY_N_TOKENS = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Used functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_summaries(dataset):\n",
    "    prefix = 'summarize: '\n",
    "    documents = []\n",
    "    summaries = []\n",
    "    ids = []\n",
    "    for i, sample in enumerate(zip(dataset['text'], dataset['summary'])):\n",
    "        for text in sample[1]:\n",
    "            documents.append(prefix + sample[0])\n",
    "            summaries.append(text['text'])\n",
    "            ids.append(i)\n",
    "    return {'ids': ids, 'text': documents, 'summary': summaries}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, text_n_tokens, summary_n_tokens):\n",
    "    model_inputs = TOKENIZER(examples['text'], max_length=text_n_tokens, padding='max_length')\n",
    "    labels = TOKENIZER(examples['summary'], max_length=summary_n_tokens, padding='max_length')\n",
    "\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    model_inputs['ids'] = examples['ids']\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_text_by_k(text, kind, k):\n",
    "    return len(text[kind]) <= k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_text(text, kind, k):\n",
    "    text[kind] = text[kind][:k]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    rouge = load(\"rouge\")\n",
    "    \n",
    "    labels = np.where(labels != -100, labels, TOKENIZER.pad_token_id)\n",
    "    predictions = np.where(predictions != -100, predictions, TOKENIZER.pad_token_id)\n",
    "\n",
    "    decoded_preds = TOKENIZER.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = TOKENIZER.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != TOKENIZER.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the booksum dataset at a chapter level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_data = load_dataset(\"ubaada/booksum-complete-cleaned\", \"chapters\")\n",
    "\n",
    "train_ds = Dataset.from_dict(extract_summaries(book_data['train']))\n",
    "test_ds = Dataset.from_dict(extract_summaries(book_data['test']))\n",
    "val_ds = Dataset.from_dict(extract_summaries(book_data['validation']))\n",
    "\n",
    "del book_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first tokenize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 9534/9534 [00:27<00:00, 342.29 examples/s]\n",
      "Map: 100%|██████████| 1485/1485 [00:03<00:00, 402.97 examples/s]\n",
      "Map: 100%|██████████| 1432/1432 [00:04<00:00, 329.14 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_train = train_ds.map(preprocess_function, batched=True, fn_kwargs={'text_n_tokens':TEXT_N_TOKENS, 'summary_n_tokens':SUMMARY_N_TOKENS})\n",
    "tokenized_validation = val_ds.map(preprocess_function, batched=True, fn_kwargs={'text_n_tokens':TEXT_N_TOKENS, 'summary_n_tokens':SUMMARY_N_TOKENS})\n",
    "tokenized_test = test_ds.map(preprocess_function, batched=True, fn_kwargs={'text_n_tokens':TEXT_N_TOKENS, 'summary_n_tokens':SUMMARY_N_TOKENS})\n",
    "\n",
    "del train_ds, val_ds, test_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And drop the tests over 1500 tokens because we don't want to use incomplete targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 9534/9534 [00:27<00:00, 352.89 examples/s]\n",
      "Filter: 100%|██████████| 1485/1485 [00:03<00:00, 398.06 examples/s]\n",
      "Filter: 100%|██████████| 1432/1432 [00:04<00:00, 357.29 examples/s]\n"
     ]
    }
   ],
   "source": [
    "usable_train = tokenized_train.filter(filter_text_by_k, fn_kwargs={'kind':'labels', 'k':SUMMARY_N_TOKENS})\n",
    "usable_validation = tokenized_validation.filter(filter_text_by_k, fn_kwargs={'kind':'labels', 'k':SUMMARY_N_TOKENS})\n",
    "usable_test = tokenized_test.filter(filter_text_by_k, fn_kwargs={'kind':'labels', 'k':SUMMARY_N_TOKENS})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see we do not loose may points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of kept train entries:  0.8794839521711768\n",
      "Ratio of kept test entries:  0.9050279329608939\n",
      "Ratio of kept validation entries:  0.9218855218855219\n"
     ]
    }
   ],
   "source": [
    "print('Ratio of kept train entries: ', len(usable_train) / len(tokenized_train))\n",
    "print('Ratio of kept test entries: ', len(usable_test) / len(tokenized_test))\n",
    "print('Ratio of kept validation entries: ', len(usable_validation) / len(tokenized_validation))\n",
    "\n",
    "del tokenized_train, tokenized_test, tokenized_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to create two different datasets: One with the texts under the previously fixed number of tokens (under_k) and other with all the texts (complete)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 8385/8385 [00:12<00:00, 680.61 examples/s]\n",
      "Map: 100%|██████████| 1369/1369 [00:01<00:00, 708.02 examples/s]\n",
      "Map: 100%|██████████| 1296/1296 [00:01<00:00, 735.16 examples/s]\n",
      "Filter: 100%|██████████| 8385/8385 [00:20<00:00, 402.70 examples/s]\n",
      "Filter: 100%|██████████| 1369/1369 [00:03<00:00, 415.65 examples/s]\n",
      "Filter: 100%|██████████| 1296/1296 [00:03<00:00, 426.05 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_complete = usable_train.map(truncate_text, fn_kwargs={'kind':'input_ids', 'k':TEXT_N_TOKENS})\n",
    "test_complete = usable_validation.map(truncate_text, fn_kwargs={'kind':'input_ids', 'k':TEXT_N_TOKENS})\n",
    "val_complete = usable_test.map(truncate_text, fn_kwargs={'kind':'input_ids', 'k':TEXT_N_TOKENS})\n",
    "\n",
    "train_under_k = usable_train.filter(filter_text_by_k, fn_kwargs={'kind':'input_ids', 'k':TEXT_N_TOKENS})\n",
    "test_under_k = usable_validation.filter(filter_text_by_k, fn_kwargs={'kind':'input_ids', 'k':TEXT_N_TOKENS})\n",
    "val_under_k = usable_test.filter(filter_text_by_k, fn_kwargs={'kind':'input_ids', 'k':TEXT_N_TOKENS})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let us have a look to the ratios of the new datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of train entries under 4000 tokens:  0.4894454382826476\n",
      "Ratio of test entries under 4000 tokens:  0.49382716049382713\n",
      "Ratio of validation entries under 4000 tokens:  0.5178962746530315\n"
     ]
    }
   ],
   "source": [
    "print(f'Ratio of train entries under {TEXT_N_TOKENS} tokens: ', len(train_under_k) / len(usable_train))\n",
    "print(f'Ratio of test entries under {TEXT_N_TOKENS} tokens: ', len(test_under_k) / len(usable_test))\n",
    "print(f'Ratio of validation entries under {TEXT_N_TOKENS} tokens: ', len(val_under_k) / len(usable_validation))\n",
    "\n",
    "del usable_train, usable_test, usable_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cropped approach, we only keep about the 60% of each dataset. In the complete option we still have the same ratio, but with truncated texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_under_k = {'train':train_under_k, 'test':test_under_k, 'val':val_under_k}\n",
    "datasets_complete = {'train':train_complete, 'test':test_complete, 'val':val_complete}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define the pretrained models and arguments that are going to be used for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=TOKENIZER, model=T5)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(T5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us set the args:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 2e-05\n",
    "weight_decay = 0.01\n",
    "acc_steps = 4\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=\"t5_model\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end = True,\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        weight_decay=weight_decay,\n",
    "        save_total_limit=2,\n",
    "        num_train_epochs=14,\n",
    "        predict_with_generate=True,\n",
    "        fp16=True,\n",
    "        gradient_accumulation_steps=acc_steps,\n",
    "        generation_max_length=128,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the approach that is going to be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = datasets_complete # in {datasets_under_k, datasets_complete}\n",
    "patience = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And create the trainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_567959/123186086.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=datasets[\"train\"],\n",
    "    eval_dataset=datasets[\"val\"],\n",
    "    tokenizer=TOKENIZER,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=patience)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first evaluate in order to check the performance of the model before fine tunning it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, it performs very poorly. We are going to train it now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='29344' max='29344' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [29344/29344 13:21:14, Epoch 13/14]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.480100</td>\n",
       "      <td>1.246521</td>\n",
       "      <td>0.187200</td>\n",
       "      <td>0.026300</td>\n",
       "      <td>0.123100</td>\n",
       "      <td>0.123100</td>\n",
       "      <td>118.907400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.430700</td>\n",
       "      <td>1.227910</td>\n",
       "      <td>0.198300</td>\n",
       "      <td>0.027700</td>\n",
       "      <td>0.126600</td>\n",
       "      <td>0.126600</td>\n",
       "      <td>121.390400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.445300</td>\n",
       "      <td>1.215685</td>\n",
       "      <td>0.204100</td>\n",
       "      <td>0.029500</td>\n",
       "      <td>0.130200</td>\n",
       "      <td>0.130200</td>\n",
       "      <td>123.133500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.419500</td>\n",
       "      <td>1.203647</td>\n",
       "      <td>0.218700</td>\n",
       "      <td>0.031500</td>\n",
       "      <td>0.135500</td>\n",
       "      <td>0.135500</td>\n",
       "      <td>126.147400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.427600</td>\n",
       "      <td>1.199493</td>\n",
       "      <td>0.216600</td>\n",
       "      <td>0.031400</td>\n",
       "      <td>0.135600</td>\n",
       "      <td>0.135700</td>\n",
       "      <td>126.184400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.404400</td>\n",
       "      <td>1.193996</td>\n",
       "      <td>0.214400</td>\n",
       "      <td>0.030900</td>\n",
       "      <td>0.136100</td>\n",
       "      <td>0.136200</td>\n",
       "      <td>126.201400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.414800</td>\n",
       "      <td>1.190361</td>\n",
       "      <td>0.214200</td>\n",
       "      <td>0.030300</td>\n",
       "      <td>0.135700</td>\n",
       "      <td>0.135800</td>\n",
       "      <td>126.297100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.398600</td>\n",
       "      <td>1.188405</td>\n",
       "      <td>0.213300</td>\n",
       "      <td>0.030600</td>\n",
       "      <td>0.135700</td>\n",
       "      <td>0.135700</td>\n",
       "      <td>126.265400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.396800</td>\n",
       "      <td>1.187252</td>\n",
       "      <td>0.217000</td>\n",
       "      <td>0.031500</td>\n",
       "      <td>0.136400</td>\n",
       "      <td>0.136400</td>\n",
       "      <td>126.276200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.390500</td>\n",
       "      <td>1.186253</td>\n",
       "      <td>0.216400</td>\n",
       "      <td>0.031500</td>\n",
       "      <td>0.136200</td>\n",
       "      <td>0.136300</td>\n",
       "      <td>126.649700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.354200</td>\n",
       "      <td>1.185902</td>\n",
       "      <td>0.215600</td>\n",
       "      <td>0.031200</td>\n",
       "      <td>0.135900</td>\n",
       "      <td>0.135900</td>\n",
       "      <td>126.626500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=29344, training_loss=1.4213041286822103, metrics={'train_runtime': 48075.5999, 'train_samples_per_second': 2.442, 'train_steps_per_second': 0.61, 'total_flos': 1.24108431949824e+17, 'train_loss': 1.4213041286822103, 'epoch': 13.998330351818725})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1296' max='1296' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1296/1296 16:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.1859015226364136,\n",
       " 'eval_rouge1': 0.2156,\n",
       " 'eval_rouge2': 0.0312,\n",
       " 'eval_rougeL': 0.1359,\n",
       " 'eval_rougeLsum': 0.1359,\n",
       " 'eval_gen_len': 126.6265,\n",
       " 'eval_runtime': 1009.8394,\n",
       " 'eval_samples_per_second': 1.283,\n",
       " 'eval_steps_per_second': 1.283,\n",
       " 'epoch': 13.998330351818725}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training did not really help with the scores, so we are going to perform a grid search in order to find the optimal parameters for our goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('t5_over_4k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"learning_rate\": [1e-5, 2e-5, 5e-5],\n",
    "    \"weight_decay\": [0.01, 0.05, 0.1],\n",
    "    \"gradient_accumulation_steps\": [4, 8, 12]\n",
    "}\n",
    "\n",
    "# Generate all combinations of hyperparameters\n",
    "param_combinations = list(product(*param_grid.values()))\n",
    "\n",
    "param_names = list(param_grid.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rouge = 0\n",
    "best_params = None\n",
    "\n",
    "for params in param_combinations:\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(T5)\n",
    "    hyperparams = dict(zip(param_names, params))\n",
    "    print(f\"Testing with parameters: {hyperparams}\")\n",
    "    \n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=\"t5_model\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end = True,\n",
    "        learning_rate=hyperparams[\"learning_rate\"],\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        weight_decay=hyperparams[\"weight_decay\"],\n",
    "        save_total_limit=2,\n",
    "        num_train_epochs=14,\n",
    "        predict_with_generate=True,\n",
    "        fp16=True,\n",
    "        gradient_accumulation_steps=hyperparams[\"gradient_accumulation_steps\"],\n",
    "        generation_max_length=128,\n",
    "    )\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=datasets[\"train\"],\n",
    "        eval_dataset=datasets[\"val\"],\n",
    "        tokenizer=TOKENIZER,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=patience)]\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    metrics = trainer.evaluate()\n",
    "    \n",
    "    # Save the best performing parameters using rouge1 as main metric\n",
    "    if metrics[\"eval_rouge1\"] > best_rouge:\n",
    "        best_rouge = metrics[\"eval_rouge1\"]\n",
    "        best_params = hyperparams\n",
    "        print(f\"New best parameters found: {best_params} with ROUGE-1: {best_rouge}\")\n",
    "        trainer.save_model('t5_best_model')\n",
    "\n",
    "print(f\"Best parameters: {best_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After doing the param grid, the best parameters found are:\n",
    "\n",
    "* 'learning_rate': 2e-05\n",
    "* 'weight_decay': 0.01\n",
    "* 'gradient_accumulation_steps': 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us have a look to the performance of the different models we have trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = datasets_complete # in {datasets_under_k, datasets_complete}\n",
    "model_name = 't5_over_4k' # in {T5, 't5_overfitted', 't5_best_model', 't5_over_4k'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/tmp/ipykernel_567959/3190820645.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1369' max='1369' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1369/1369 17:26]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.214807391166687,\n",
       " 'eval_model_preparation_time': 0.0015,\n",
       " 'eval_rouge1': 0.2082,\n",
       " 'eval_rouge2': 0.0304,\n",
       " 'eval_rougeL': 0.133,\n",
       " 'eval_rougeLsum': 0.1329,\n",
       " 'eval_gen_len': 125.5405,\n",
       " 'eval_runtime': 1062.4515,\n",
       " 'eval_samples_per_second': 1.289,\n",
       " 'eval_steps_per_second': 1.289}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    \n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=model_name,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end = True,\n",
    "    learning_rate=2e-05,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=14,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=4,\n",
    "    generation_max_length=128,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=datasets[\"train\"],\n",
    "    eval_dataset=datasets[\"test\"],\n",
    "    tokenizer=TOKENIZER,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=patience)]\n",
    ")\n",
    "    \n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(datasets['test']['labels'][0], dtype=torch.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = TOKENIZER(datasets['test']['text'][10], return_tensors=\"pt\", truncation=True).input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "inputs = inputs.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(inputs, max_new_tokens=100, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Esther's story is about the happiest of the happy years she has been the mistress of Bleak House. The couple gave her darling into her arms, and through many weeks I never left her. The little child who was to have done so much was born before the turf was planted on its father's grave. The help that my dear counted on did come to her, though it came in the eternal wisdom, for another purpose. The help that my\""
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOKENIZER.decode(outputs[0], skip_special_tokens=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
